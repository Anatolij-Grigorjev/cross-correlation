1. All those words have their innate positions within the text , meaning that we can order them all relative to each other within their individual sentences . (SCORE: 97.82110899787372)
12. Given the emotional charge of vocabulary words in the abstract , removed from a specific speaker or context , we can extrapolate the emotion a text is trying to convey by analyzing what sequences and patterns these initial words are arranged into by the text author . (SCORE: 148.86177736582502)
13. As stated above , such arrangement of words , which are common text tokens , into a graph will not preserve any context information about the text , it will not let determine what the text was about ( in general ) . (SCORE: 101.50128942279639)
14. This means , that the results of such a model can be compared with those of a model that includes a context component , to determine , which is more accurate or faster at its job . (SCORE: 95.88240631809741)
15. An example of a context-aware model could be the structuring of the same token graph , but using the words from context-forming sentences as the origins for the calculations ( context-forming sentences can be found via Major Topic Detection [8] , for example ) . (SCORE: 175.93140736370148)
17. The concept is entirely virtual - it does not affect the graph and exists entirely as help for the researcher to have a starting point . (SCORE: 224.28872846026556)
