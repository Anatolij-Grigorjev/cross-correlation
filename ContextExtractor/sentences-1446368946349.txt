1. All those words have their innate positions within the text , meaning that we can order them all relative to each other within their individual sentences . (SCORE: 100.27499143023324)
12. Given the emotional charge of vocabulary words in the abstract , removed from a specific speaker or context , we can extrapolate the emotion a text is trying to convey by analyzing what sequences and patterns these initial words are arranged into by the text author . (SCORE: 151.84795268868064)
13. As stated above , such arrangement of words , which are common text tokens , into a graph will not preserve any context information about the text , it will not let determine what the text was about ( in general ) . (SCORE: 100.49447694063514)
14. This means , that the results of such a model can be compared with those of a model that includes a context component , to determine , which is more accurate or faster at its job . (SCORE: 96.01906751149262)
15. An example of a context-aware model could be the structuring of the same token graph , but using the words from context-forming sentences as the origins for the calculations ( context-forming sentences can be found via Major Topic Detection [8] , for example ) . (SCORE: 130.96645978500788)
17. This is required , because it would be naive to assume , that all or even most texts will have word tokens that only appear at the start of sentences and can therefore be placed on one side of the graph as an easy starting point . (SCORE: 139.12692842457383)
